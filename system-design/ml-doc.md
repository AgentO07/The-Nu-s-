# ML Overview (Incomplete) (Review Needed)

 This documentation serves as a reference to bridge the knowledge gap discrepancy between the members of the team. When this documentation refers to "Journaly" it is explicitly talking about the feedback-review product that we will sell to enterprise.

## Introduction

 The concept is straightforward: create a system that captures reviews, performs sentiment analysis and Named Entity Recognition (NER), and then maps the emotions to the entities.

 - What the hell is NER?

 Named Entity Recognition (NER) is a technique that helps identify and categorize key information (keywords) in text. For example, "Apple is releasing a new iPhone in San Francisco," NER would help identify "Apple" as a company, "iPhone" as a product, and "San Francisco" as a location. [Look here for spacy's documentation on linguistics.]("https://spacy.io/usage/linguistic-features)

 - Ok, how do we do this?
 
 At first glance, it looks straightforward: obtain any form of media (like voice or text reviews), then perform sentiment analysis on it. After that, apply Named Entity Recognition (NER) to **generate** key terms and then link the keywords to the sentiments identified, showing which keywords aroused which emotions in the customer.

 - That's not too bad, just give me the models for both and I'll do it ;)

 That is where the hard part comes in, by nature modern machine learning models are bound to fail. Sometimes, the probability that the model gives an output that is **"correct"** is worse than a fair coin toss. the word "correct" here depends. We can't use LLM's to drive a car. But the chances of someone figuring out a way to do it via LLM's are non-zero. 

 Machine learning models generally perform better when the prompts or inputs they receive are similar to the data they were trained on. This is because the model's ability to generate accurate and relevant responses or predictions depends on how well it can recognize and generalize patterns from its training data. There are practical    limitations to this so I won't go into that. But it should be enough for the reader to understand that modern ML models **will** fail.

 - I think you need to look up "prompt engineering"

 Lol

 - What do we do now?

  The simplest way to approach this without needing a more advanced model (duh) is to run multiple of them. They should be similar enough that you can rely on any one of them if the others fail, yet they should be different enough that all of them don't fail on the **SAME** data.

  - So we're running extra models as a backup plan? The heck?

  While that's one of the reasons, for **general inputs** (the data the model was designed to handle), there is a much better chance that **ALL** models will perform ok. In this case, the additional models will COMPLEMENT the results produced by the other models. This will become much clearer later.

 - How and what models will we use to do this?

 Read the next section.


 
## Brief Overview of the Technologies Used

We are using **seven** different types of models (might change later). Some of them don't have to do anything with the "backup" plan

 - Voice to text (Transcribes voice to text)
 - Voice to emotion (Sentiment analysis on text)
 - Text to "correct" sentences (Splits the chunk of text into sentences)
 - Text to emotion (Sentiment analysis on text)
 - NER (Keywords basically)
 - Binary result model (classifies text as either positive or negative, that's it)
 - A Rating model (out of five)

 [IMPORTANT TODO - Add additional details about these]

## Structure of the output generated by the enterprise-models.

For the enterprise, there will be five files instead of four. The only file I will be talking about is the one that is most worth talking about; The enterprise one. The json on running the models look something like this,

```
{
    "rating": {
        "label": "1 star",
        "score": 0.7004992961883545
    },
    "sentences": [
        {
            "sentence": "Honestly, these guys have the audacity to raise prices on flagships for a screen like that?",
            "keywords": [
                "prices",
                "screen",
                "flagships",
                "audacity",
                "guys"
            ],
            "binary": {
                "label": "NEGATIVE",
                "score": 0.999441921710968
            },
            "emotion": {
                "anger": 0.532305896282196,
                "disgust": 0.31351104378700256,
                "surprise": 0.08709196746349335,
                "sadness": 0.006636465433984995,
                "fear": 0.005771156866103411,
                "joy": 0.003903308417648077,
                "neutral": 0.05078011006116867
            }
        }, ..
```

This is output of the first sentence of a long review. There is actually more sentences but that wouldn't make sense to put it here and my point will come across anyways. Now let's make some sense of it. 

The "rating" is the model trying to answer what a person would rate the product or whatever out of five stars. It is the only model that doesn't run on each sentence, instead it parses the ENTIRE review and rates it out of five. So it runs once.

Let's talk about the keywords, there are a BUNCH of them, the model is giving more keywords than what is appropriate. What do we do? Well, I wanted to show you how EASY it is to classify keywords into Nouns, Proper Nouns, Verbs, Adjectives etc. We can simply do this by:

```keywords = [token.text for token in nlp_object if token.pos_ in {"NOUN", "PROPN"}]```

Tadaa!

We can pass a bunch of arguments(NOUN, PROPN) here inside the curly braces. And it'll include them in the keywords. Look up the link I sent above for more of these. A very insightful [read](https://spacy.io/usage/linguistic-features).

For the binary, there's nothing much to say really. They're just useful for ```+ve``` and ```-ve``` of the keywords. But really they server the most important purpose of all from the business front atleast. They help us decide whether the "keywords" were actually said in a negative, or a positive connotation.

Notice, there is nothing like ```{'label': 'anger', 'score': 0.532305896282196},{'label': 'disgust', 'score': 0.31351104378700256}``` etc., we use the emotions itself as the keys and map it to the values, so when we're progamatically accessing them for future use, it wouldn't be annoying. It also looks good. ```"anger" : score``` looks way way better than the one previously imo.

[The rest are to be done later]

## CODE 

